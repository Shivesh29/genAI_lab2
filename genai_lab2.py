# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kz5O2RktG4jKzP-Knc0ZO3yYIwFWYEwg
"""

# ==============================
# Improved GAN on MNIST (No Input)
# ==============================

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torchvision.utils import save_image
import os

# ------------------------------
# Fixed Configuration (Lab-safe)
# ------------------------------
dataset_choice = "mnist"
epochs = 50
batch_size = 64
noise_dim = 100
learning_rate = 0.0002
save_interval = 5

# ------------------------------
# Device
# ------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ------------------------------
# Output Folders
# ------------------------------
os.makedirs("generated_samples", exist_ok=True)
os.makedirs("final_generated_images", exist_ok=True)

# ------------------------------
# Dataset & Normalization
# ------------------------------
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # [-1, 1]
])

dataset = datasets.MNIST(
    root="./data",
    train=True,
    download=True,
    transform=transform
)

dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# ------------------------------
# Generator
# ------------------------------
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(noise_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, 784),
            nn.Tanh()
        )

    def forward(self, z):
        return self.net(z).view(-1, 1, 28, 28)

# ------------------------------
# Discriminator
# ------------------------------
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(784, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

G = Generator().to(device)
D = Discriminator().to(device)

# ------------------------------
# Loss & Optimizers
# ------------------------------
criterion = nn.BCELoss()
opt_G = optim.Adam(G.parameters(), lr=learning_rate, betas=(0.5, 0.999))
opt_D = optim.Adam(D.parameters(), lr=learning_rate, betas=(0.5, 0.999))

# ------------------------------
# Training Loop
# ------------------------------
for epoch in range(1, epochs + 1):
    d_loss_epoch = 0
    g_loss_epoch = 0
    correct = 0

    for real_imgs, _ in dataloader:
        real_imgs = real_imgs.to(device)
        batch = real_imgs.size(0)

        real_labels = torch.ones(batch, 1).to(device)
        fake_labels = torch.zeros(batch, 1).to(device)

        # ---- Train Discriminator ----
        opt_D.zero_grad()

        real_out = D(real_imgs)
        loss_real = criterion(real_out, real_labels)

        noise = torch.randn(batch, noise_dim).to(device)
        fake_imgs = G(noise)
        fake_out = D(fake_imgs.detach())
        loss_fake = criterion(fake_out, fake_labels)

        d_loss = loss_real + loss_fake
        d_loss.backward()
        opt_D.step()

        # ---- Train Generator ----
        opt_G.zero_grad()
        output = D(fake_imgs)
        g_loss = criterion(output, real_labels)
        g_loss.backward()
        opt_G.step()

        d_loss_epoch += d_loss.item()
        g_loss_epoch += g_loss.item()

        correct += ((real_out > 0.5).sum().item() + (fake_out < 0.5).sum().item())

    d_acc = 100 * correct / (2 * len(dataset))

    print(
        f"Epoch {epoch}/{epochs} | "
        f"D_loss: {d_loss_epoch/len(dataloader):.4f} | "
        f"D_acc: {d_acc:.2f}% | "
        f"G_loss: {g_loss_epoch/len(dataloader):.4f}"
    )

    # ---- Save Samples ----
    if epoch % save_interval == 0:
        with torch.no_grad():
            z = torch.randn(25, noise_dim).to(device)
            samples = G(z)
            save_image(
                samples,
                f"generated_samples/epoch_{epoch:02d}.png",
                nrow=5,
                normalize=True
            )

# ------------------------------
# Final Image Generation
# ------------------------------
print("\nGenerating final 100 images...")
with torch.no_grad():
    z = torch.randn(100, noise_dim).to(device)
    final_imgs = G(z)
    for i, img in enumerate(final_imgs):
        save_image(
            img,
            f"final_generated_images/img_{i:03d}.png",
            normalize=True
        )

print("âœ… Training complete.")
print("ğŸ“ Check folders:")
print(" - generated_samples/")
print(" - final_generated_images/")

!zip -r lab2_outputs.zip generated_samples final_generated_images